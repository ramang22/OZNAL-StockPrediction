## Pôvod dát

Naše dáta o jednotlivých príspevkov z redditu pochádzajú z dvoch datasetov zo stránky keggle.com

1. Posts - https://www.kaggle.com/gpreda/reddit-wallstreetsbets-posts
2. Comments - https://www.kaggle.com/mattpodolak/rwallstreetbets-posts-and-comments

Dáta o cene jednotlivých akcií sa nám podarilo získať z [yahoo finance](https://finance.yahoo.com/), pomocou python balíčka [yfinance](https://pypi.org/project/yfinance/).

### Posts data

```{r}
df <- read.csv("../data/raw_data/posts.csv")
summary(df)
```

Dataset o príspevkoch obsauje 47 000 záznamov a má 8 stĺpcov. 

* Title - text titulku príspevku

* Score - skóre príspevku na fóre

* Id - identifikátor príspevku

* url - url na príspekov

* comms_num - počet komentárov pod príspevkom

* created - timestamp, kedy bol daný príspevok vytvorený

* body - telo príspevku

* timestamp - timestamp

**Head postov vyzerá nasledovne:**
![](assets/posts_head_raw.png)

### Comments data

```{r}
# Zakomentovali sme tento riadok lebo hrozne dlho trva nacitanie 4gb dat...
# namiesto vypisu sme pouzili len screenshot
#df <- read.csv("../data/raw_data/comments.csv")
#summary(df)
print("Odkomentovat")
```
![](assets/comments_summary.png)

Dataset comments obsahuje 9.5 milióna záznamov pričim má 37 stĺpcov. V nasledujúcej časti opíšeme len stĺpce, s ktorýme sme pracovali pri tvorbe datasetu.

**Použité stĺpce :**

* body - Text komentáru

* parent_id - identifikátor jednotlivých postov

* id - id komentára

* created_utc - čas keby bol daný komentár uverejnený

* score - ohodnotenie daného komentára

## Spracovanie dát

Spracovanie dát je jediná časť projektu, v ktorej sme sa rozhodli využiť Python namiesto Rka a to z dôvodu že v pythone sme skúsenejší a vieme pracovať rýchlejšie a efektívnejšie. Všetky python notebooky sú zverejnené v zložke python_code. Práve efektivita bola potrebná pri takomto množstve dát, ktoré sme museli spracovať. Pre rýchlejšie spracovanie dát sme využili python rozhranie pre [Apache Spark](https://spark.apache.org/) - [pyspark](https://spark.apache.org/docs/latest/api/python/). Taktiež sme využili google cloud computing platformu, na ktorej sme spúštali pySpark aby sme mali k dispozícii väčší výpočtový výkon. 

Pre zaujímavosť celé trvanie spracovania dát na google cloud computing platforme trvalo približne 10 hodín. 

**Spracovanie dát vyzeralo nasledovne:**

1. Najskôr sme jednotlivé datasety vyčistili od nepotrebných stĺpcov a zredukovali počet dát len na záznamy od 1.1.2019 a neskôr. Chceme pracovat len v tomto časovom úseku lebo práve v tomto čase sa daný subreddit stal populárnym. Celý tento cyklus je reprezentovaný v prvom python súbore.

2. Ako druhý krok prišla práca v pySparku. Pre veľký objem dát sme sa rozhodli využiť naše skúsenosti so Spark pipeline implementáciou. Pôvodne sme potrebovali dáta transformovať tak, aby vznikol nový stĺpec obsahujúci symboly akcií spomenutých v danom texte. K tomuto sme vytvorili Python Notebook (spark_pipeline.ipnyb), ktorý s využitím Apache Spark serveru tokenizoval jednotlivé texty datasetu a porovnával ich so všetkými známymi symbolmi akcií. Po prvej konzultácii sme sa rozhodli túto map reduce pipeline rozšíriť tak aby analyzovala sentiment jednotlivých tokenov a vytvárala tak celkový sentiment každého riadku datasetu. K tomuto sme sa rozhodli z dôvodu nedostatočného výpočtového výkonu využiť cluster na Google Cloud Platforme. Spark job riešiaci túto úlohu sa nachádza v súbore spark_job.py. S využitím clustra (1 master 4 slaves) trvala celková transformácia datasetu približne 6 hodín.  

3. Nakoniec sme museli dataset komentárov a príspevkov spojiť do jedného. Zvolili sme nasledujúci systém. Každý stock symbol môže byť reprezentovaný v datasete len raz v jednom konkrétnom dni. Takže sme počet riadkov zredukovali na $max\_num\_rows = ticker\_count * num\_dates$. Pričom sa jednotlivé parametre tickerov sčítavajú. Potom sme priradili stock market stĺpce k jednotlivým ďnom a tickerom a takto nám vznikol dataset. Túto poslednú časť môžete nájst v druhom python súbore.  

Obrázok znázorňuje grafiky akým cyklom museli prejsť jednotlivé dáta.
![](assets/sparkcycle.png)


**vzniknutý dataset po všetkých úpravach vyzeral nasledovne:**

```{r}
df <- read.csv("../data/final_spark.csv")
summary(df)
```
Následne bolo potrebné vypočítať ďalšie dôležité stĺpce. Na ktoré sme už použili kód v Rku.

```{r}
#Taktiež sme tento blok kódu zakomentovali z dôvodu, dlhého vytvárania dokumentu.

# df['spread'] <- round(df['High'] / df['Low'] - 1,digits=3 )
# df['change'] <- round(df['Close'] / df['Open'] - 1,digits=3 )
# df['peak'] <- round(df['High'] / df['Open'] - 1,digits=3 )
# df['trough'] <- round(df['Open'] / df['Low'] - 1,digits=3 )
# df['marketCap'] <- round(df['Close'] * df['float'] ,digits=3 )
# df$date <- as.Date(df$date)
# df$min7 <- 0
# df$max7 <- 0
# df$min30 <- 0
# df$max30 <- 0
# df$max7plus <- 0
# df$min7plus <- 0
# df$max7plus <- 0
# df$min7plus <- 0
# 
# f <- function(x) {
#  symbol = x['symbol']
#   date <- as.Date(x['date'])
#   startDate = date - 7
#   endDate = date
#   df7 <- df$High[df$symbol == symbol & df$date >= startDate & df$date <= endDate]
#   max7 <-  max(df7)
#   min7 <-  min(df7)
# 
#   startDate = date - 30
#   df30 <- df$High[df$symbol == symbol & df$date >= startDate & df$date <= endDate]
#   max30 <-  max(df30)
#   min30 <-  min(df30)
# 
#  startDate = date
#   endDate = date + 7
#   df30 <- df$High[df$symbol == symbol & df$date >= startDate & df$date <= endDate]
#   max7plus <-  max(df30)
#   min7plus <-  min(df30)
#   c(max7, min7, max30, min30,max7plus, min7plus )
# }

# results <- apply(df, 1,f)
# df$min7 <- results[2,]
# df$max7 <- results[1,]
# df$min30 <- results[4,]
# df$max30 <- results[3,]
# df$max7plus <- results[5,]
# df$min7plus <- results[6,]
# head(df)
# write.csv(df,file="../data/dataset_final.csv")
```
Novo vziknuté stĺpce sú dôležité pre analyzovanie stock marketu. Tieto stĺpce sa považujú za industry standard pre sledovanie pohybu jednotlivých akcií. Na konci tohoto spracovania dát dostávame náš finálny dataset, s ktorým pracujeme v analýze a nasledujúcich krokoch.
